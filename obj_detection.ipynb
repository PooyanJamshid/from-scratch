{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8958677",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "650d3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torchvision.ops.roi_pool import RoIPool\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94fb9f",
   "metadata": {},
   "source": [
    "# Faster R-CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416d99f",
   "metadata": {},
   "source": [
    "Faster R-CNN was introduced in 2015 by Shaoqing Ren and his colleagues [link](https://arxiv.org/abs/1506.01497).\n",
    "The previous R-CNN algorithm used selective search to generate region proposals and processed each ROI separately on the image, which was very time-consuming.\n",
    "Faster R-CNN, however, performs region proposal on the feature map instead of the original image.\n",
    "Moreover, it uses a Region Proposal Network (RPN) instead of traditional/manual methods.\n",
    "The RPN is implemented as a neural network, which allows end-to-end learning and significantly improves localization accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6d79cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_extract(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super().__init__()\n",
    "        model=models.resnet50(pretrained=True)\n",
    "        self.model=nn.Sequential(*list(model.children())[:-2])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.model(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50af428",
   "metadata": {},
   "source": [
    "In this implementation, we used ResNet-50 and removed the last two layers, which were originally designed for classification, because our goal is to generate feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "991189c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rpn(nn.Module):\n",
    "    def __init__(self, inchanel,midchanel,anchors):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Conv2d(inchanel,midchanel,kernel_size=3,padding=2)\n",
    "        self.cls=nn.Conv2d(midchanel,anchors*2,1)\n",
    "        self.bbox=nn.Conv2d(midchanel,anchors*4,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        cls=self.cls(x)\n",
    "        box=self.bbox(x)\n",
    "        return cls,box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb98729",
   "metadata": {},
   "source": [
    "In RPN, we place small fixed anchors on the feature map and assign them fixed sizes.\n",
    "Then, using the cls output, we estimate the probability of an object existing in each anchor.\n",
    "(Each anchor corresponds to a region relative to the original image.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b72e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class head(nn.Module):\n",
    "    def __init__(self,inchanel,num_anchor,num_class):\n",
    "        super().__init__()\n",
    "        self.roi=RoIPool(output_size=(7,7),spatial_scale=1/16)\n",
    "        self.fc1=nn.Linear(inchanel*7*7,1024)\n",
    "        self.fc2=nn.Linear(1024,1024)\n",
    "        self.cls=nn.Linear(1024,num_anchor*num_class)\n",
    "        self.bbox=nn.Linear(1024,num_anchor*4)\n",
    "\n",
    "    def forward(self,x,rois):\n",
    "        x=self.roi(x,rois)\n",
    "        x = x.view(x.size(0), -1)  #flatten\n",
    "        x=self.fc1(x)\n",
    "        x=self.fc2(x)\n",
    "        cls=self.cls(x)\n",
    "        bbox=self.bbox(x)\n",
    "        return cls,bbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60288164",
   "metadata": {},
   "source": [
    "In the Head, each ROI is cropped from the feature map and resized to a fixed size.\n",
    "It then passes through two fully connected layers (FC).\n",
    "Finally, the network predicts the bounding box coordinates and the class p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e9dc8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class faster_rcnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature=feature_extract()\n",
    "        self.rpn=rpn(2048,512,9)\n",
    "        self.head=head(2048,9,3)\n",
    "\n",
    "    def forward(self,image,rois):\n",
    "        feature=self.feature(image)\n",
    "        rpn_cls,rpn_box=self.rpn(feature)\n",
    "        cls_head,cls_box=self.head(feature,rois)\n",
    "        return rpn_cls, rpn_box, cls_head, cls_box\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc03d1f",
   "metadata": {},
   "source": [
    "RPN: looks at several small regions of the image (anchors) and says, \"there might be an object here or not.\"\n",
    "\n",
    "It does not yet know the objectâ€™s class or its precise coordinates.\n",
    "\n",
    "Head (Fast R-CNN): works on the ROIs selected by the RPN and predicts the exact bounding box and the actual class of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "303600bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPN logits: torch.Size([2, 18, 9, 9])\n",
      "RPN bbox: torch.Size([2, 36, 9, 9])\n",
      "RCNN class logits: torch.Size([2, 27])\n",
      "RCNN bbox: torch.Size([2, 36])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = faster_rcnn()  \n",
    "images = torch.randn(2, 3, 224, 224)\n",
    "rois = torch.tensor([[0, 10, 10, 100, 100], [1, 50, 50, 150, 150]], dtype=torch.float)  # [batch_idx, x1, y1, x2, y2]\n",
    "    \n",
    "rpn_logits, rpn_bbox, cls_logits, bbox_reg = model(images, rois)\n",
    "print(\"RPN logits:\", rpn_logits.shape)\n",
    "print(\"RPN bbox:\", rpn_bbox.shape)\n",
    "print(\"RCNN class logits:\", cls_logits.shape)\n",
    "print(\"RCNN bbox:\", bbox_reg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55368c93",
   "metadata": {},
   "source": [
    "# Yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f45a7",
   "metadata": {},
   "source": [
    "In 2015, Joseph Redmon introduced the YOLO (You Only Look Once) model. Unlike two-stage architectures such as Faster R-CNN, which first generate region proposals and then perform classification and regression on them, YOLO performs detection in a single stage (single-shot) and in one pass through the network. This approach is very fast and suitable for real-time applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a53c5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class yolo(nn.Module):\n",
    "    def __init__(self, s,c,b):\n",
    "        super().__init__()\n",
    "        self.s=s\n",
    "        self.b=b\n",
    "        self.c=c\n",
    "       # model=models.resnet50(pretrained=True)\n",
    "        self.feature=nn.Sequential(\n",
    "            nn.Conv2d(3,16,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,3),\n",
    "            nn.MaxPool2d(2,2),       \n",
    "        )\n",
    "\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*self.s**2,516),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,self.s**2*(self.b*4+c))\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.feature(x)\n",
    "        x=self.classifier(x)\n",
    "        x=x.view(-1,self.s,self.s,self.b*5+self.c)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c59ba77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 111, 111])\n"
     ]
    }
   ],
   "source": [
    "model = yolo(s=7, c=20, b=2)\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    out = model.feature(x)\n",
    "print(out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0359ccf6",
   "metadata": {},
   "source": [
    "# SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e01ac",
   "metadata": {},
   "source": [
    "The SSD (Single Shot MultiBox Detector) model was introduced in 2016 by Google researchers under the supervision of Wei Liu. This model is a single-shot architecture, meaning that object detection is performed in one step and directly.\n",
    "\n",
    "Unlike YOLO (which is anchor-free in its newer versions), SSD is an anchor-based model. It can be considered conceptually between Faster R-CNN and YOLO:\n",
    "\n",
    ". Like Faster R-CNN, it uses anchor boxes,\n",
    "\n",
    ". But like YOLO, it is single-stage and faster.\n",
    "\n",
    "The structure of SSD consists of two main parts:\n",
    "\n",
    "1.Backbone for extracting feature maps\n",
    "\n",
    "2.Prediction layers (classification + localization) for generating class scores and bounding box coordinates at multiple scales\n",
    "\n",
    "In terms of speed, SSD is faster than Faster R-CNN, but slightly slower than YOLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23df6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ssd_head(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.anchors=4\n",
    "        self.classes=4\n",
    "        self.feature=nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(3,16,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16,32,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64,kernel_size=3),\n",
    "            nn.ReLU()\n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.feature(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95ef8fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD(nn.Module):\n",
    "    def __init__(self,anchor=4,classes=3):\n",
    "        super().__init__()\n",
    "        self.feature=ssd_head()\n",
    "\n",
    "        self.classes=nn.Conv2d(\n",
    "            64,anchor*classes,kernel_size=3\n",
    "        )\n",
    "\n",
    "        self.anchors=nn.Conv2d(64,anchor*4,kernel_size=3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.feature(x)\n",
    "        classes=self.classes(x)\n",
    "        anchor=self.anchors(x)\n",
    "        return classes,anchor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06ba8837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes output shape: torch.Size([2, 12, 106, 106])\n",
      "Anchors output shape: torch.Size([2, 16, 106, 106])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "model = SSD(anchor=4, classes=3)\n",
    "\n",
    "classes_out, anchors_out = model(x)\n",
    "\n",
    "print(\"Classes output shape:\", classes_out.shape)\n",
    "print(\"Anchors output shape:\", anchors_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2dc7ea",
   "metadata": {},
   "source": [
    "# RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e776f76",
   "metadata": {},
   "source": [
    "RetinaNet was introduced by the Facebook AI Research team and performs well in detecting small objects. Like SSD and YOLO, it is a single-stage detector, meaning it predicts both the location and class of objects in one step.\n",
    "One cool feature of RetinaNet is the focal loss, which helps the model focus more on hard examples during training.\n",
    "The architecture also includes two subnetworks: one for classification and another for box regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e83e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet=models.resnet18(pretrained=True)\n",
    "        self.back=nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.out_channels=512\n",
    "\n",
    "    def forward(self,x):\n",
    "        back=self.back(x)\n",
    "        return back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0e3139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ratinehead(nn.Module):\n",
    "    def __init__(self,in_chanel,num_anchors,num_class):\n",
    "        super().__init__()\n",
    "        cls_subnet=[]\n",
    "        anchor_subnet=[]\n",
    "        for _ in range(5):\n",
    "            cls_subnet.append(nn.Conv2d(in_chanel,in_chanel,3,padding=1))\n",
    "            cls_subnet.append(nn.ReLU())\n",
    "        \n",
    "        self.cls_subnet=nn.Sequential(*cls_subnet)\n",
    "\n",
    "        for _ in range(5):\n",
    "            anchor_subnet.append(nn.Conv2d(in_chanel,in_chanel,3,padding=1))\n",
    "            anchor_subnet.append(nn.ReLU())\n",
    "        \n",
    "        self.anchor_subnet=nn.Sequential(*anchor_subnet)\n",
    "\n",
    "        self.cls=nn.Conv2d(in_chanel,num_anchors*num_class,kernel_size=3,padding=1)\n",
    "        self.anchors=nn.Conv2d(in_chanel,num_anchors*4,kernel_size=3,padding=1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        cls_subnet=self.cls_subnet(x)\n",
    "        anchor_subnet=self.anchor_subnet(x)\n",
    "\n",
    "        cls=self.cls(cls_subnet)\n",
    "        anchor=self.anchors(anchor_subnet)\n",
    "        return cls,anchor\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a449b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = self.alpha * (1 - pt) ** self.gamma * ce\n",
    "        return loss.mean()\n",
    "\n",
    "class RetinaNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone()\n",
    "        self.head = ratinehead(self.backbone.out_channels, num_anchors=9,num_class=num_classes)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        feats = self.backbone(x)\n",
    "        cls_outs, reg_outs = self.head(feats)\n",
    "        if targets is None:\n",
    "            return cls_outs, reg_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7847761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cls output shape: torch.Size([2, 27, 7, 7])\n",
      "Reg output shape: torch.Size([2, 36, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model = RetinaNet(num_classes=3)\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "cls_outs, reg_outs = model(x)\n",
    "print(\"Cls output shape:\", cls_outs.shape)    # [B, 9*C, H, W]\n",
    "print(\"Reg output shape:\", reg_outs.shape)    # [B, 9*4, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c68834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
