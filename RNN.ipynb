{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xiEOSjhU62Ys"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8cWuUXEik5h"
      },
      "source": [
        "### In this model, due to the limited resources I had, I wasnâ€™t able to provide the model with all the necessary data, nor could I train it for the required number of epochs. But since the structure is correct, itâ€™s easy to run this process anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7reghXLzMuAm"
      },
      "outputs": [],
      "source": [
        "text='''I am Harry Potter. I am a young boy who lives with my aunt and uncle, the Dursleys, in a small house on Privet Drive. I am curious about the world and I often wonder why I feel different from others. One day, I receive a letter from Hogwarts School of Witchcraft and Wizardry. I am surprised and excited because I learn that I am a wizard. I have never seen magic before, and I am eager to discover all the wonders that await me. I pack my things and travel to Hogwarts, feeling nervous and thrilled at the same time. I meet many students on the train and I make friends with Ron Weasley and Hermione Granger. I am happy to have friends who understand me and share my excitement. We explore the castle together and discover hidden rooms, secret passages, and magical creatures. I see dragons, hippogriffs, house-elves, unicorns, and other creatures I have never imagined. I learn many spells and practice them every day. I try simple charms, defensive spells, and more powerful magic. I am careful when using magic because I want to do things correctly and safely. I attend many classes at Hogwarts, including Potions, Defense Against the Dark Arts, Transfiguration, Herbology, and Flying Lessons. I play Quidditch on a broomstick and practice teamwork with my friends. I learn strategy, courage, and cooperation. I face many challenges and adventures. I discover the secret of the Philosopherâ€™s Stone and protect it from those who want to steal it. I fight trolls, solve puzzles, and confront dark wizards. I learn that courage, loyalty, and friendship are more important than any spell. I help my friends when they are in danger and I protect those who need help. I practice magic every day and read books to improve my knowledge. I observe magical creatures and learn how to care for them. I explore the castle and the grounds, discovering new places and hidden secrets. I meet ghosts, magical plants, and enchanted objects. I try to understand their powers and how to use them responsibly. I grow stronger and wiser with every year at Hogwarts. I learn about the history of magic and the ongoing fight between good and evil. I discover that love and loyalty are powerful forms of magic that cannot be defeated. I practice spells, study lessons, and learn life lessons. I make mistakes, but I try to learn from them. I act with courage even when situations are dangerous or uncertain. I share my knowledge with friends and help them overcome challenges. I remember the advice of my teachers and follow guidance when necessary. I explore secret corridors, hidden rooms, and mysterious passages. I encounter magical creatures that teach me new things about the world. I care for my friends and help them when they are in trouble. I feel proud when we succeed together and celebrate small victories. I understand that honesty, bravery, and kindness are stronger than any magic spell. I practice patience and persistence, even when learning is difficult. I know that helping others is more important than personal gain. I continue to explore Hogwarts and the magical world. I meet new students, learn new spells, and discover magical plants and objects. I face challenges that test my courage, intelligence, and compassion. I try to be responsible and protect others whenever possible. I grow more confident and wise each year. I think before I act and trust my instincts. I understand that mistakes are lessons and challenges are opportunities. I feel proud of my achievements and grateful for my friends and teachers. I continue to practice magic, learn new spells, and explore the magical world. I believe that even ordinary people can do extraordinary things if they believe in themselves and support others. I am Harry Potter, a young wizard learning about magic, friendship, courage, love, and responsibility. I am ready to face new adventures, help my friends, and protect the magical world. I am determined to be brave, kind, and wise. I am learning, exploring, and growing every day. I discover new things about magic, about people, and about myself. I face dangers, solve mysteries, and meet magical creatures. I practice spells, play Quidditch, and attend lessons. I learn teamwork, responsibility, and courage. I help friends, protect Hogwarts, and try to do what is right. I continue to discover secrets of the castle, learn new magic, and improve my skills. I face dark forces and learn how to fight them. I understand that courage, hope, and friendship are more powerful than fear. I act with kindness, loyalty, and bravery in every situation. I continue to grow as a wizard and as a person. I explore, learn, and practice magic every day. I believe in myself and in the power of my friends. I know that together we can overcome obstacles and achieve extraordinary things. I am ready for new challenges, new lessons, and new adventures. I continue to write my story at Hogwarts, learning from each day, and sharing experiences with friends. I discover the importance of love, courage, and perseverance. I help others when they need me and I protect those who cannot protect themselves. I practice magic responsibly, learn from mistakes, and grow stronger every day. I am Harry Potter, a wizard who is learning about life, magic, friendship, and courage. I am ready to face the world with bravery, wisdom, and kindness. I continue to learn, explore, and grow, discovering the magical world and my place in it.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svv6qlN6XzZj"
      },
      "source": [
        "# Free runing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEPhNvEAik5k"
      },
      "source": [
        "In this model, we use the modelâ€™s own output â€” meaning the word that the model itself generates â€” as the input for the next layer. This mechanism can be seen as a form of learning from its own predictions and can be converted for training purposes. However, in the real world, it is rarely used and serves a different purpose ðŸ˜…."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sfK8nW6MUFmT"
      },
      "outputs": [],
      "source": [
        "class rnn_free_runing(nn.Module):\n",
        "    def __init__(self,text,embeded_size,):\n",
        "        super().__init__()\n",
        "        #defining variables\n",
        "        self.text=text\n",
        "        self.words=text.split()\n",
        "        self.vocab=list(set(self.words))\n",
        "        #change word to idx and idx to word\n",
        "        self.words2idx={word:i for i,word in enumerate(self.vocab)}\n",
        "        self.idx2word={i:word for i,word in enumerate(self.vocab)}\n",
        "        self.embeded_size=embeded_size\n",
        "\n",
        "        # defining weights of words\n",
        "        self.embeding=nn.Embedding(embedding_dim=embeded_size,num_embeddings=len(self.vocab))\n",
        "        # defin weight of network\n",
        "\n",
        "        #input weight\n",
        "        self.whx = nn.Parameter(torch.randn(embeded_size, embeded_size))\n",
        "        #hidden state weight\n",
        "        self.whh = nn.Parameter(torch.randn(embeded_size,embeded_size))\n",
        "        #hidden-to-output weight\n",
        "        self.why = nn.Parameter(torch.randn(embeded_size,len(self.vocab)))\n",
        "        #hidden state bias\n",
        "        self.bh=nn.Parameter(torch.randn(embeded_size))\n",
        "        #bias of output\n",
        "        self.by=nn.Parameter(torch.randn(len(self.vocab)))\n",
        "        self.relu=nn.ReLU()\n",
        "        self.sofmax=nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,len_words):\n",
        "        #This matrix is responsible for transferring information\n",
        "        #from the previous layer to the next\n",
        "        ht = torch.ones(1, self.embeded_size)\n",
        "\n",
        "        #start word\n",
        "        start_word=self.words[0]\n",
        "\n",
        "        #get idx of start word\n",
        "        start_word_idx=self.words2idx[start_word]\n",
        "        #conver to tensor\n",
        "        start_word_idx=torch.tensor(start_word_idx)\n",
        "\n",
        "        #We list every word generated by the model\n",
        "        #in this text to form a complete sentence\n",
        "        sentences=[]\n",
        "\n",
        "\n",
        "        #get weight of start word\n",
        "        embeded=self.embeding(start_word_idx)\n",
        "\n",
        "        for i in range(len_words):\n",
        "\n",
        "\n",
        "            at=(embeded @ self.whx) + (ht @ self.whh)+self.bh\n",
        "            ht=self.relu(at)\n",
        "\n",
        "            output=self.by + (ht @ self.why)\n",
        "\n",
        "            idx_output=torch.argmax(output)\n",
        "\n",
        "\n",
        "\n",
        "            #in this line of code,\n",
        "            #the output from the model is converted to serve as input for the next layer\n",
        "            embeded=self.embeding(idx_output)\n",
        "\n",
        "\n",
        "            sentences.append(output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return torch.cat(sentences,dim=0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0aHcldpGSt2Z"
      },
      "outputs": [],
      "source": [
        "#splited text\n",
        "words=text.split()\n",
        "\n",
        "vocab=list(set(words))\n",
        "\n",
        "words_idx={word:i for i,word in enumerate(vocab)}\n",
        "idx_word={i:word for i,word in enumerate(vocab)}\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#objected model\n",
        "model_free_runing=rnn_free_runing(text=text,embeded_size=100).to(device)\n",
        "#adam optimizer\n",
        "optim=torch.optim.Adam(model_free_runing.parameters(),lr=1e-4)\n",
        "\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "\n",
        "#get idx of all words of text\n",
        "idxes=[words_idx[word] for i,word in enumerate(words)]\n",
        "#convert to tensor\n",
        "idxes=torch.tensor(idxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0BrQgEFik5l"
      },
      "source": [
        "Because cross-entropy itself applies softmax and log on its input, I didnâ€™t apply argmax on the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZdhMT9KSumm",
        "outputId": "b8c460b7-a012-4dae-9fa5-80a9d467c6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.48it/s]\n"
          ]
        }
      ],
      "source": [
        "train_loss=[]\n",
        "#start to training\n",
        "model_free_runing.train()\n",
        "for i in tqdm.trange(5):\n",
        "\n",
        "        data=idxes.to(device).long()\n",
        "        optim.zero_grad()\n",
        "        predict=model_free_runing(len(words))\n",
        "\n",
        "\n",
        "\n",
        "        #I didnâ€™t apply softmax, log, or argmax because\n",
        "        #the CrossEntropy function calculates all of these on the logits\n",
        "\n",
        "\n",
        "        loss=loss_fn(predict,data)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        train_loss.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckeBpkdUSw3_",
        "outputId": "f22cb807-2b29-42e6-f38d-d7b917d064f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dursleys,',\n",
              " 'Dursleys,',\n",
              " 'Dursleys,',\n",
              " 'Dursleys,',\n",
              " 'Dursleys,',\n",
              " 'Dursleys,',\n",
              " 'Dursleys,',\n",
              " 'Dursleys,',\n",
              " 'Dursleys,',\n",
              " 'Dursleys,']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  logits = model_free_runing.forward(10)\n",
        "  logits=torch.argmax(logits,dim=1)\n",
        "  word=[idx_word[int(i)] for i in logits]\n",
        "word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44mvPld9X8gy"
      },
      "source": [
        "# Teacher Forsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X05WvGIPik5m"
      },
      "source": [
        "In this model, unlike the free-running algorithm, we donâ€™t use the modelâ€™s own output as the input for the next time step. Instead, we use the correct word as input. This structure is more practical in industry, easier to train, and better covers the intended task. Of course, it is also more structured and logical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DPPnbwrnU0jh"
      },
      "outputs": [],
      "source": [
        "class rnn_teacher_forecing(nn.Module):\n",
        "    def __init__(self, text,embeded_size):\n",
        "        super().__init__()\n",
        "        #This part is exactly the free-running section of the model\n",
        "\n",
        "        self.text=text\n",
        "\n",
        "        self.words=text.split()\n",
        "        self.vocab=list(set(self.words))\n",
        "        self.word2idx={word:i for i,word in enumerate(self.vocab)}\n",
        "        self.idx2word={i:word for i,word in enumerate(self.vocab)}\n",
        "        self.embeded_size=embeded_size\n",
        "\n",
        "        self.embeding=nn.Embedding(embedding_dim=embeded_size,num_embeddings=len(self.vocab))\n",
        "\n",
        "\n",
        "\n",
        "        self.whx = nn.Parameter(torch.randn(embeded_size, embeded_size))\n",
        "        self.whh = nn.Parameter(torch.randn(embeded_size,embeded_size))\n",
        "        self.why = nn.Parameter(torch.randn(embeded_size,len(self.vocab)))\n",
        "        self.bh=nn.Parameter(torch.randn(embeded_size))\n",
        "        self.by=nn.Parameter(torch.randn(len(self.vocab)))\n",
        "        self.relu=nn.ReLU()\n",
        "        self.sofmax=nn.Softmax(dim=1)\n",
        "        self.ht=torch.ones(1,embeded_size)\n",
        "\n",
        "    def forward(self,len_words):\n",
        "            ht=torch.ones(1,self.embeded_size)\n",
        "            word=self.words[0]\n",
        "            sentences=[]\n",
        "            word_idx=self.word2idx[word]\n",
        "\n",
        "\n",
        "            for i in range(len_words-1):\n",
        "\n",
        "                word_idx=torch.tensor(word_idx)\n",
        "                embeded=self.embeding(word_idx)\n",
        "                at=(embeded@self.whx) + ht @ self.whh\n",
        "\n",
        "                ht=self.relu(at)\n",
        "\n",
        "                output=self.by + (ht @ self.why)\n",
        "\n",
        "\n",
        "                sentences.append(output)\n",
        "\n",
        "                #here, I provide the correct word for the next time step,\n",
        "                # not the word generated by the model.\n",
        "\n",
        "                word_idx=self.word2idx[self.words[i+1]]\n",
        "\n",
        "\n",
        "\n",
        "            return torch.cat(sentences,dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX9pzWLIik5m"
      },
      "source": [
        "Because cross-entropy itself applies softmax and log on its input, I didnâ€™t apply argmax on the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Yy9GtwC3WrG3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "words=text.split()\n",
        "\n",
        "vocab=list(set(words))\n",
        "words_idx={word:i for i,word in enumerate(vocab)}\n",
        "idx_word={i:word for i,word in enumerate(vocab)}\n",
        "\n",
        "#device\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_teacher_forecing=rnn_teacher_forecing(text=text,embeded_size=200).to(device)\n",
        "\n",
        "#adam optimizer\n",
        "optim=torch.optim.Adam(model_teacher_forecing.parameters(),lr=1e-4)\n",
        "\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "\n",
        "idxes=[words_idx[word] for i,word in enumerate(words)]\n",
        "idxes=torch.tensor(idxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mcKtr2fvWtDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e8a95e-172d-484f-d718-51aa423a40c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:07<00:00,  2.81it/s]\n"
          ]
        }
      ],
      "source": [
        "train_loss=[]\n",
        "model_teacher_forecing.train()\n",
        "for i in tqdm.trange(20):\n",
        "    data=idxes.to(device).long()\n",
        "    optim.zero_grad()\n",
        "    predict=model_free_runing(len(words))\n",
        "\n",
        "\n",
        "\n",
        "    loss=loss_fn(predict,data)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    train_loss.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "n4CzaNDzWxaA"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  logits = model_teacher_forecing.forward(10)\n",
        "\n",
        "\n",
        "sentences=torch.argmax(logits,dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXGUHpsZW18h",
        "outputId": "c932ca35-f663-4c2b-d7c0-7f523f0f9b1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['try', 'at', 'about', 'try', 'places', 'places', 'care', 'about', 'each']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "[idx_word[int(i)] for i in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85poow2Eik5n"
      },
      "source": [
        "From a structural perspective, as I mentioned before, it is completely clear that teacher forcing is better than free-running."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}