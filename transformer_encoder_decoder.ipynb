{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "73f0907b",
      "metadata": {
        "id": "73f0907b"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c44b70f0",
      "metadata": {
        "id": "c44b70f0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as f\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab4195b",
      "metadata": {},
      "source": [
        "In this model, unlike the free-running algorithm, we don’t use the model’s own output as the input for the next time step. Instead, we use the correct word as input. This structure is more practical in industry, easier to train, and better covers the intended task. Of course, it is also more structured and logical."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d11009c",
      "metadata": {
        "id": "3d11009c"
      },
      "source": [
        "## text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e4ce33db",
      "metadata": {
        "id": "e4ce33db"
      },
      "outputs": [],
      "source": [
        "text='''I am Harry Potter. I am a young boy who lives with my aunt and uncle, the Dursleys, in a small house on Privet Drive. I am curious about the world and I often wonder why I feel different from others. One day, I receive a letter from Hogwarts School of Witchcraft and Wizardry. I am surprised and excited because I learn that I am a wizard. I have never seen magic before, and I am eager to discover all the wonders that await me. I pack my things and travel to Hogwarts, feeling nervous and thrilled at the same time. I meet many students on the train and I make friends with Ron Weasley and Hermione Granger. I am happy to have friends who understand me and share my excitement. We explore the castle together and discover hidden rooms, secret passages, and magical creatures. I see dragons, hippogriffs, house-elves, unicorns, and other creatures I have never imagined. I learn many spells and practice them every day. I try simple charms, defensive spells, and more powerful magic. I am careful when using magic because I want to do things correctly and safely. I attend many classes at Hogwarts, including Potions, Defense Against the Dark Arts, Transfiguration, Herbology, and Flying Lessons. I play Quidditch on a broomstick and practice teamwork with my friends. I learn strategy, courage, and cooperation. I face many challenges and adventures. I discover the secret of the Philosopher’s Stone and protect it from those who want to steal it. I fight trolls, solve puzzles, and confront dark wizards. I learn that courage, loyalty, and friendship are more important than any spell. I help my friends when they are in danger and I protect those who need help. I practice magic every day and read books to improve my knowledge. I observe magical creatures and learn how to care for them. I explore the castle and the grounds, discovering new places and hidden secrets. I meet ghosts, magical plants, and enchanted objects. I try to understand their powers and how to use them responsibly. I grow stronger and wiser with every year at Hogwarts. I learn about the history of magic and the ongoing fight between good and evil. I discover that love and loyalty are powerful forms of magic that cannot be defeated. I practice spells, study lessons, and learn life lessons. I make mistakes, but I try to learn from them. I act with courage even when situations are dangerous or uncertain. I share my knowledge with friends and help them overcome challenges. I remember the advice of my teachers and follow guidance when necessary. I explore secret corridors, hidden rooms, and mysterious passages. I encounter magical creatures that teach me new things about the world. I care for my friends and help them when they are in trouble. I feel proud when we succeed together and celebrate small victories. I understand that honesty, bravery, and kindness are stronger than any magic spell. I practice patience and persistence, even when learning is difficult. I know that helping others is more important than personal gain. I continue to explore Hogwarts and the magical world. I meet new students, learn new spells, and discover magical plants and objects. I face challenges that test my courage, intelligence, and compassion. I try to be responsible and protect others whenever possible. I grow more confident and wise each year. I think before I act and trust my instincts. I understand that mistakes are lessons and challenges are opportunities. I feel proud of my achievements and grateful for my friends and teachers. I continue to practice magic, learn new spells, and explore the magical world. I believe that even ordinary people can do extraordinary things if they believe in themselves and support others. I am Harry Potter, a young wizard learning about magic, friendship, courage, love, and responsibility. I am ready to face new adventures, help my friends, and protect the magical world. I am determined to be brave, kind, and wise. I am learning, exploring, and growing every day. I discover new things about magic, about people, and about myself. I face dangers, solve mysteries, and meet magical creatures. I practice spells, play Quidditch, and attend lessons. I learn teamwork, responsibility, and courage. I help friends, protect Hogwarts, and try to do what is right. I continue to discover secrets of the castle, learn new magic, and improve my skills. I face dark forces and learn how to fight them. I understand that courage, hope, and friendship are more powerful than fear. I act with kindness, loyalty, and bravery in every situation. I continue to grow as a wizard and as a person. I explore, learn, and practice magic every day. I believe in myself and in the power of my friends. I know that together we can overcome obstacles and achieve extraordinary things. I am ready for new challenges, new lessons, and new adventures. I continue to write my story at Hogwarts, learning from each day, and sharing experiences with friends. I discover the importance of love, courage, and perseverance. I help others when they need me and I protect those who cannot protect themselves. I practice magic responsibly, learn from mistakes, and grow stronger every day. I am Harry Potter, a wizard who is learning about life, magic, friendship, and courage. I am ready to face the world with bravery, wisdom, and kindness. I continue to learn, explore, and grow, discovering the magical world and my place in it.'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dbb5890",
      "metadata": {
        "id": "5dbb5890"
      },
      "source": [
        "## parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "11d7f048",
      "metadata": {
        "id": "11d7f048"
      },
      "outputs": [],
      "source": [
        "vocab_size=len(list(set(text.split())))\n",
        "embed_size=96\n",
        "max_len=10\n",
        "num_head=3\n",
        "batch_size=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36ea43e",
      "metadata": {
        "id": "f36ea43e"
      },
      "source": [
        "## Embedings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "369fe792",
      "metadata": {
        "id": "369fe792"
      },
      "outputs": [],
      "source": [
        "token_embeding=nn.Embedding(vocab_size,embed_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "28392cd1",
      "metadata": {
        "id": "28392cd1"
      },
      "outputs": [],
      "source": [
        "def pe(x_len,embed_size,batch_size):\n",
        "\n",
        "    pe=torch.zeros(batch_size,x_len,embed_size)\n",
        "\n",
        "    for j in range(x_len):\n",
        "        for i in range(embed_size):\n",
        "            #for even index -> sin function\n",
        "            if i%2 == 0:\n",
        "                pe[:,:,i]=math.sin(j / (10000 ** (i / embed_size)))\n",
        "\n",
        "\n",
        "            #for odd index -> cos function\n",
        "            if i%2 == 1:\n",
        "                pe[:,:,i]=math.cos(j / (10000 ** (i / embed_size)))\n",
        "\n",
        "\n",
        "\n",
        "    return (pe)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8bac7b29",
      "metadata": {
        "id": "8bac7b29"
      },
      "outputs": [],
      "source": [
        "torch_sequze=torch.tensor([1,2,3,4])\n",
        "token_embed=token_embeding(torch_sequze)\n",
        "#positional embeding\n",
        "pe_embedind=pe(x_len=len(torch_sequze),embed_size=embed_size,batch_size=batch_size)\n",
        "\n",
        "x=token_embed+pe_embedind"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5ede31a",
      "metadata": {
        "id": "e5ede31a"
      },
      "source": [
        "## transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "69d1e3c8",
      "metadata": {
        "id": "69d1e3c8"
      },
      "outputs": [],
      "source": [
        "class Transformer_encoder_Block(nn.Module):\n",
        "    def __init__(self,embed_size,num_head,batch_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.batch_size=batch_size\n",
        "        self.num_head=num_head\n",
        "        self.embed_size=embed_size\n",
        "        # dimentional of each head\n",
        "        self.head_dim = embed_size // num_head\n",
        "\n",
        "        self.q=nn.Linear(embed_size,embed_size)#query\n",
        "        self.k=nn.Linear(embed_size,embed_size)#keys\n",
        "        self.v=nn.Linear(embed_size,embed_size)#values\n",
        "\n",
        "        self.o=nn.Linear(embed_size,embed_size)#output\n",
        "\n",
        "        #normalization\n",
        "        self.norm1=nn.LayerNorm(embed_size)\n",
        "        self.norm2=nn.LayerNorm(embed_size)\n",
        "\n",
        "        #feed forward\n",
        "        self.ffn=nn.Linear(embed_size,embed_size)\n",
        "\n",
        "        self.softmax=nn.Softmax(dim=-1)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        B,S,_=x.shape\n",
        "\n",
        "        head_dim=self.head_dim\n",
        "\n",
        "        Q=self.q(x).view(B,self.num_head,S,self.head_dim)\n",
        "        V=self.v(x).view(B,self.num_head,S,self.head_dim)\n",
        "        K=self.k(x).view(B,self.num_head,S,self.head_dim)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        score=torch.matmul(Q,K.transpose(-2,-1))/ torch.sqrt(torch.tensor(self.head_dim))\n",
        "        score=self.softmax(score)\n",
        "        #attention score\n",
        "        score=score@V\n",
        "\n",
        "        #here finish the attention fomula\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        batch,seq_lengh,num_head_score,_=score.transpose(1,2).shape\n",
        "        # concatenate all heads\n",
        "\n",
        "        score_norm=score.view(batch,seq_lengh,num_head_score* _)\n",
        "        score_norm = self.o(score_norm)\n",
        "        #norma;ization and residual conection\n",
        "\n",
        "        score_norm=self.norm1(score_norm+x)\n",
        "\n",
        "        ffn=self.ffn(score_norm)\n",
        "\n",
        "\n",
        "        output=self.norm2(ffn+x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dd4ff03b",
      "metadata": {
        "id": "dd4ff03b"
      },
      "outputs": [],
      "source": [
        "class Transformer_encoder(nn.Module):\n",
        "    def __init__(self, embed_size,num_head,batch_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(\n",
        "            [Transformer_encoder_Block(embed_size, num_head,batch_size) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, y):\n",
        "        for layer in self.layers:\n",
        "\n",
        "            y = layer(y)\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b2729642",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2729642",
        "outputId": "60f37629-e1ec-4087-f0e0-00e02805a6c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 96])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer_encoder=Transformer_encoder(embed_size,num_head,batch_size=1,num_layers=3)\n",
        "encoder_output=transformer_encoder(x)\n",
        "encoder_output.shape\n",
        "# encoder shape output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d27fe83c",
      "metadata": {
        "id": "d27fe83c"
      },
      "source": [
        "## tranformer decpder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f7a9e2f9",
      "metadata": {
        "id": "f7a9e2f9"
      },
      "outputs": [],
      "source": [
        "class Transformer_decoder_Block(nn.Module):\n",
        "\n",
        "    def __init__(self,num_head,embed_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_head=num_head\n",
        "        self.embed_size=embed_size\n",
        "        self.dk=embed_size//num_head\n",
        "\n",
        "        #weights of attention\n",
        "\n",
        "        self.wq=nn.Linear(embed_size,embed_size)#query\n",
        "        self.wk=nn.Linear(embed_size,embed_size)#keys\n",
        "        self.wv=nn.Linear(embed_size,embed_size)#values\n",
        "        self.wo = nn.Linear(embed_size,embed_size)\n",
        "\n",
        "        # weights of cross attention\n",
        "        self.wq_cross=nn.Linear(embed_size,embed_size)#query\n",
        "        self.wk_cross=nn.Linear(embed_size,embed_size)#keys\n",
        "        self.wv_cross=nn.Linear(embed_size,embed_size)#values\n",
        "\n",
        "\n",
        "        self.ffn1=nn.Linear(embed_size,embed_size)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.ffn2=nn.Linear(embed_size,embed_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.norm1=nn.LayerNorm(embed_size)\n",
        "        self.norm2=nn.LayerNorm(embed_size)\n",
        "        self.norm3=nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.softmax=nn.Softmax(dim=-1)\n",
        "\n",
        "    def mask(self, S, device):\n",
        "      #make First superdiagonal matrix and 0 is false other is true\n",
        "      # model attend to true values\n",
        "      mask = torch.triu(torch.ones(S, S, device=device), diagonal=1).bool()\n",
        "      return mask\n",
        "\n",
        "    def forward(self,x_encoder,x_decoder):\n",
        "\n",
        "        B,S,_=x_encoder.shape\n",
        "\n",
        "        Q=self.wq(x_decoder).view(B,self.num_head,S,self.dk)\n",
        "        V=self.wv(x_decoder).view(B,self.num_head,S,self.dk)\n",
        "        K=self.wk(x_decoder).view(B,self.num_head,S,self.dk)\n",
        "\n",
        "        score=torch.matmul(Q,K.transpose(-2,-1))/torch.sqrt(torch.tensor(self.dk))\n",
        "        # masked after words with -inf\n",
        "        mask_matrix = self.mask(S, x_decoder.device)\n",
        "        score = score.masked_fill(mask_matrix, float('-inf'))\n",
        "\n",
        "        score=self.softmax(score)\n",
        "        score=score@V\n",
        "        # finish attention formula\n",
        "\n",
        "        #concatenate heads\n",
        "        concat_heads = score.transpose(1,2).contiguous().view(B, S, self.num_head*self.dk)\n",
        "        attention_output=self.wo(concat_heads)\n",
        "\n",
        "        #normalization and residual conection\n",
        "\n",
        "        attention_output=self.norm1(attention_output+x_decoder)\n",
        "\n",
        "        Q_cross=self.wq_cross(x_decoder).view(B,self.num_head,S,self.dk)\n",
        "        V_cross=self.wv_cross(x_encoder).view(B,self.num_head,S,self.dk)\n",
        "        K_cross=self.wk_cross(x_decoder).view(B,self.num_head,S,self.dk)\n",
        "\n",
        "        score_cross=torch.matmul(Q_cross,K_cross.transpose(-2,-1))/torch.sqrt(torch.tensor(self.dk))\n",
        "        score_cross=self.softmax(score_cross)\n",
        "        score_cross=score_cross@V_cross\n",
        "        #cross attention this is attend to decoder input for example:\n",
        "        #for translation we heave to give model second token embeding words\n",
        "        \n",
        "        dec_output=self.norm2(score_cross.view(B,S,self.num_head*self.dk)+x_decoder)\n",
        "        ffn_1=self.ffn1(dec_output)\n",
        "        ffn_1=self.relu(ffn_1)\n",
        "        ffn_2=self.ffn2(ffn_1)\n",
        "        norm2=self.norm3(ffn_2+x_decoder)\n",
        "        return norm2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "943614f4",
      "metadata": {
        "id": "943614f4"
      },
      "outputs": [],
      "source": [
        "class transformer_decoder(nn.Module):\n",
        "    def __init__(self,num_head, embed_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(\n",
        "            [Transformer_decoder_Block(num_head,embed_size) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, encoder_output,decoder_input):\n",
        "        for layer in self.layers:\n",
        "\n",
        "            decoder_input= layer(encoder_output,decoder_input)\n",
        "\n",
        "        return decoder_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b498976b",
      "metadata": {
        "id": "b498976b"
      },
      "outputs": [],
      "source": [
        "model_decoder=Transformer_decoder_Block(num_head,embed_size)\n",
        "output_decoder=model_decoder(encoder_output,x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30ae88bb",
      "metadata": {
        "id": "30ae88bb"
      },
      "source": [
        "## tranformer for generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ceafdc10",
      "metadata": {
        "id": "ceafdc10"
      },
      "outputs": [],
      "source": [
        "class generate_text_transformer(nn.Module):\n",
        "    def __init__(self,num_head,embed_size,num_encoder_blocks,num_decoder_blocks,batch_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder_model=Transformer_encoder(embed_size=embed_size,num_head=num_head,batch_size=batch_size,num_layers=num_encoder_blocks)\n",
        "        self.decoder_model=transformer_decoder(embed_size=embed_size,num_head=num_head,num_layers=num_decoder_blocks)\n",
        "\n",
        "    def forward(self,y):\n",
        "        encoder_output=self.encoder_model(y)\n",
        "\n",
        "        decoder_output=self.decoder_model(encoder_output,y)\n",
        "\n",
        "        return decoder_output\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cd1603e6",
      "metadata": {
        "id": "cd1603e6"
      },
      "outputs": [],
      "source": [
        "transformer_generate=generate_text_transformer(num_head,embed_size,2,2,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718b12e8",
      "metadata": {
        "id": "718b12e8"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0a9358da",
      "metadata": {
        "id": "0a9358da"
      },
      "outputs": [],
      "source": [
        "word_split=list(set(text.split()))\n",
        "word2idx={word:i for i,word in enumerate(word_split)}\n",
        "idx2word={i:word for i,word in enumerate(word_split)}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "abcebc03",
      "metadata": {
        "id": "abcebc03"
      },
      "outputs": [],
      "source": [
        "sequeze=[word2idx[i] for i in word_split[:4]]\n",
        "\n",
        "sequeze=torch.tensor(sequeze)\n",
        "\n",
        "token_embed=token_embeding(torch_sequze)\n",
        "pe_embedind=pe(x_len=len(torch_sequze),embed_size=embed_size,batch_size=batch_size)\n",
        "\n",
        "#tokens for started model\n",
        "first_x=token_embed+pe_embedind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c9e2801a",
      "metadata": {
        "id": "c9e2801a"
      },
      "outputs": [],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transformer_generate_model=transformer_generate=generate_text_transformer(num_head,embed_size,1,1,1).to(device)#num en-de coder blocks=1 too batch_size\n",
        "\n",
        "optim=torch.optim.Adam(lr=1e-4,params=transformer_generate_model.parameters())\n",
        "\n",
        "loss_fn=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5733cb19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5733cb19",
        "outputId": "886e3dd7-d1cb-4b8e-abe4-fc5f29ad014c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:05<00:00,  1.82it/s]\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "transformer_generate.train()\n",
        "train_loss=[]\n",
        "\n",
        "for epoch in tqdm.trange(10):\n",
        "    x=first_x.to(device)\n",
        "    optim.zero_grad()\n",
        "    for i in range(len(word_split)-1):\n",
        "\n",
        "\n",
        "\n",
        "      x=transformer_generate(x)\n",
        "\n",
        "\n",
        "      #Detaching the computation graph to control gradient flow\n",
        "\n",
        "      x=x.detach()\n",
        "      token_embeding=token_embeding.to(device)\n",
        "\n",
        "      words_x_weight=x@token_embeding.weight.T\n",
        "      #last word\n",
        "      last_word_weigh=words_x_weight[:,-1,:]\n",
        "\n",
        "      #next word is target word\n",
        "\n",
        "      target=( torch.tensor(word2idx[word_split[i+1]]).long()).to(device)\n",
        "\n",
        "\n",
        "      loss=loss_fn(last_word_weigh,target.unsqueeze(0))\n",
        "\n",
        "      # get index \n",
        "\n",
        "      predict=torch.argmax(last_word_weigh,dim=-1)\n",
        "\n",
        "      #teacher forcing learning\n",
        "      predict=token_embeding(predict)\n",
        "      predict=predict.view(1,1,-1)\n",
        "      \n",
        "\n",
        "      x=torch.cat((x,predict),dim=1)\n",
        "\n",
        "\n",
        "      train_loss.append(loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "91c00fd5",
      "metadata": {
        "id": "91c00fd5"
      },
      "outputs": [],
      "source": [
        "# saved weighs\n",
        "torch.save(transformer_generate.state_dict(), \"transformer_generate.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff1f7595",
      "metadata": {},
      "source": [
        "## Genarate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "dde67365",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dde67365",
        "outputId": "a15e6538-bf4e-4f16-a9cd-d545a71c2a8c"
      },
      "outputs": [],
      "source": [
        "#load the weghits on model\n",
        "\n",
        "#transformer_generate_model=transformer_generate=generate_text_transformer(num_head,embed_size,1,1,batch_size)# num en de coder blocks =1\n",
        "#weights=torch.load('/content/transformer_generate.pth',map_location=device)\n",
        "#transformer_generate_model.load_state_dict(weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "krLxT5aPOYHf",
      "metadata": {
        "id": "krLxT5aPOYHf"
      },
      "outputs": [],
      "source": [
        "transformer_generate_model=transformer_generate_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "AaAnlsk2MLM-",
      "metadata": {
        "id": "AaAnlsk2MLM-"
      },
      "outputs": [],
      "source": [
        "transformer_generate.eval()\n",
        "senteces_preict=[]\n",
        "\n",
        "token_embeding=token_embeding.to(device)\n",
        "x=first_x.to(device)\n",
        "for i in range(5):\n",
        "\n",
        "\n",
        "\n",
        "      x=transformer_generate(x)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      words_x_weight=x@token_embeding.weight.T\n",
        "      last_word_weigh=words_x_weight[:,-1,:]\n",
        "\n",
        "\n",
        "\n",
        "      predict=torch.argmax(last_word_weigh,dim=-1)\n",
        "\n",
        "      senteces_preict.append(idx2word[predict.item()])\n",
        "      \n",
        "  \n",
        "      predict=token_embeding(predict)\n",
        "   \n",
        "      \n",
        "      predict=predict.view(1,1,-1)\n",
        "  \n",
        "\n",
        "\n",
        "      #shape: 1 , 4+n , 96\n",
        "      x=torch.cat((x,predict),dim=1)\n",
        "\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "TiPe36XYM27C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiPe36XYM27C",
        "outputId": "badedb14-aaad-4a6f-b59e-d022c39e7719"
      },
      "outputs": [],
      "source": [
        "#senteces_preict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ad2b11",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f45add9",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "73f0907b",
        "3d11009c",
        "5dbb5890",
        "f36ea43e",
        "e5ede31a",
        "d27fe83c",
        "30ae88bb"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
